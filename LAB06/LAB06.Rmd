---
title: "HW06"
author: "Smirnov"
date: "04 12 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Linear regression

## LOAD DATASET
Загружаем наши данные. Избавляемся от выбросов и берём log от income.
```{r load,echo=TRUE}
data <- read.csv('zeta_nodupes.csv')
data <- subset(data, data$education < 18 & data$education > 8 & data$income < 200000 & data$income > 10000)
data$log_income <- log10(data$income)
head(data)
```
## Зависимость income от age
Тут мы видим что был построен график:
y = 4.7 - 0.002x
```{r 1 1,echo=TRUE}
modelAge <- lm(log_income ~ age, data=data)
print(modelAge)
```
Тут мы видим параметры нашей модели, такие как t_value,P_value,R^2,F-statistic.
Прочитать про них можно на http://r-statistics.co/Linear-Regression.html
```{r 1 2,echo=TRUE}
summary(modelAge)
```

```{r plot 1,echo=TRUE}
plot(data$age, data$log_income)
abline(modelAge)
abline(h=mean(data$log_income))
```
```{r plot 2,echo=TRUE}
library(hexbin)
hexbinplot(data$log_income ~ data$age)
```
В итоге мы получили плохую модель, а значит возраст плохо предсказывает доход.
## Create new model
## New regression and predict log_income with education
```{r 2 1,echo=TRUE}
modelEducation <- lm(log_income ~ education, data=data)
print(modelEducation)
```
```{r 2 2,echo=TRUE}
summary(modelEducation)
```
```{r 2 3,echo=TRUE}
plot(data$education, data$log_income)
abline(modelEducation)
abline(h=mean(data$log_income))
```
```{r plot 3,echo=TRUE}
hexbinplot(data$log_income ~ data$education)
```

# Multiple regression model

```{r 3 1,echo=TRUE}
multipleModel <- lm(log_income ~ age + education + employment, data=data)
print(multipleModel)
```

```{r 3 2,echo=TRUE}
summary(multipleModel)
```
Residuals vs.Fitted: you want to make sure that the errors are evenly distributed over the entire range of fitted values; if the errors are markedly larger (or biased either positively or negatively) in some range of the data, this is evidence that this model may not be entirely appropriate for the problem.

Q-Q plot: tests whether or not the errors are in fact distributed approximately normally (as the model formulation assumes). If they are, the Q-Q plot will be along the x=y line. If they aren't, the model may still be adequate, but perhaps a more robust modeling method is suitable. Also, the usual diagnostics (R-squared, t-tests for significance) will not be valid.

Scale-Location: a similar idea to Residuals v. Fitted; you want to make sure that the variance (or its stand-in, "scale") is approximately constantover the range of fitted values.

Residuals vs.Leverage: used for identifying potential outliers and "influential" points. Points that are far from the centroid of the scatterplot in the x direction (high leverage) are influential, in the sense that they may have disproportionate influence on the fit (that doesn't mean they are wrong, necessarily). Points that are far from the centroid in the y direction (large residuals) are potential outliers.
```{r 3 3,echo=TRUE}
plot(multipleModel)
```

```{r 3 4,echo=TRUE}
testData <- data.frame (age= c(3,3,4,4), education = c(5,6,9,10), employment = c(4,4,9,9))
logPredictions <- predict(multipleModel,newdata=testData,type="response")
logPredictions
```
```{r 3 5,echo=TRUE}
predictions <- 10**logPredictions
```
